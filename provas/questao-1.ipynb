{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, f_act='sigmoid', n_hidden=15, l2=0.0, epochs=20, lr=0.001, shuffle=True, minibatch_size=1, seed=None):\n",
    "        self.f_act = f_act\n",
    "        self.random = np.random.RandomState(seed)\n",
    "        self.n_hidden = n_hidden\n",
    "        # Lambda value for L2-regularization.\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatch_size = minibatch_size\n",
    "    \n",
    "    # activation_function (sigmoid)\n",
    "    def _sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(z, -250, 250)))\n",
    "        \n",
    "    # activation_function (tanh)\n",
    "    def _tanh(self, z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def _relu(self, z):\n",
    "        return max(z, 0)\n",
    "        \n",
    "    def _forward(self, X):\n",
    "        # net input of hidden layer\n",
    "        z_hidden = np.dot(X, self.weight_hidden)\n",
    "        \n",
    "        # activation of hidden layer\n",
    "        if self.f_act == 'relu':\n",
    "            activation_hidden = self._relu(z_hidden)\n",
    "        elif self.f_act == 'tanh':\n",
    "            activation_hidden = self._tanh(z_hidden)\n",
    "        else:\n",
    "            activation_hidden = self._sigmoid(z_hidden)\n",
    "        \n",
    "        # net input of out layer\n",
    "        z_out = np.dot(activation_hidden, self.weight_out)\n",
    "        \n",
    "        # activation of output layer\n",
    "        if self.f_act == 'relu':\n",
    "            activation_hidden = self._relu(z_hidden)\n",
    "        elif self.f_act == 'tanh':\n",
    "            activation_out = self._tanh(z_out)\n",
    "        else:\n",
    "            activation_out = self._sigmoid(z_out)\n",
    "        \n",
    "        return z_hidden, activation_hidden, z_out, activation_out\n",
    "        \n",
    "    def _compute_cost(self, y_enc, output):\n",
    "        l2_term = self.l2 * (np.sum(self.weight_hidden ** 2.0) + np.sum(self.weight_out ** 2.0))\n",
    "        \n",
    "        term_1 = -y_enc * np.log(output)\n",
    "        term_2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term_1 - term_2) + l2_term\n",
    "        \n",
    "        return 1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # only z_out matters\n",
    "        z_out = self._forward(X)[2]\n",
    "        y_pred = np.argmax(z_out, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    def _onehot(self, y, n_classes):\n",
    "        onehot = np.zeros((n_classes, y.shape[0]))\n",
    "        for idx, val in enumerate(y.astype(int)):\n",
    "            onehot[val, idx] = 1.\n",
    "        return onehot.T\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        n_output = np.unique(y_train).shape[0]\n",
    "        n_features = X_train.shape[1]\n",
    "        \n",
    "        self.weight_hidden = np.array([[2, 1 , 1], [1, -2, 2]])\n",
    "        self.weight_hidden = self.weight_hidden.astype(np.float64)\n",
    "        \n",
    "        print(self.weight_hidden)\n",
    "        \n",
    "        self.weight_out = np.array([[-1], [3], [2]])\n",
    "        self.weight_out = self.weight_out.astype(np.float64)\n",
    "        \n",
    "        print(self.weight_out)\n",
    "        \n",
    "        # new y encoded\n",
    "        y_train_enc = self._onehot(y_train, n_output)\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            \n",
    "            if self.shuffle:\n",
    "                self.random.shuffle(indices)\n",
    "            \n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "                \n",
    "                z_hidden, activation_hidden, z_out, activation_out = self._forward(X_train[batch_idx])\n",
    "\n",
    "                # backpropragation starts here\n",
    "                \n",
    "                sigma_out = activation_out - y_train_enc[batch_idx]\n",
    "                \n",
    "                # when using sigmoid as activation function\n",
    "                if self.f_act == 'relu':\n",
    "                    activation_derivate_hidden = activation_hidden\n",
    "                elif self.f_act == 'tanh':\n",
    "                    activation_derivate_hidden = (1.0 - (activation_hidden ** 2))\n",
    "                else:\n",
    "                    # sigmoid\n",
    "                    activation_derivate_hidden = activation_hidden * (1.0 - activation_hidden)\n",
    "                \n",
    "                sigma_hidden = np.dot(sigma_out, self.weight_out.T) * activation_derivate_hidden\n",
    "                grad_weight_hidden = np.dot(X_train[batch_idx].T, sigma_hidden)\n",
    "                grad_weight_out = np.dot(activation_hidden.T, sigma_out)\n",
    "                \n",
    "                # Regularization and weight updates\n",
    "                \n",
    "                # in hidden layer\n",
    "                delta_weight_hidden = (grad_weight_hidden + self.l2 * self.weight_hidden)\n",
    "                self.weight_hidden -= self.lr * delta_weight_hidden\n",
    "\n",
    "                # in output layer\n",
    "                delta_weight_out = (grad_weight_out + self.l2 * self.weight_out)\n",
    "                self.weight_out -= self.lr * delta_weight_out\n",
    "                \n",
    "                print(self.weight_hidden)\n",
    "                print(self.weight_out)\n",
    "            \n",
    "            # for each epoch, after backpropagation ends evaluate training\n",
    "            z_hidden, activation_hidden, z_out, activation_out = self._forward(X_train)\n",
    "            \n",
    "            cost = self._compute_cost(y_enc=y_train_enc, output=activation_out)\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  1.  1.]\n",
      " [ 1. -2.  2.]]\n",
      "[[-1.]\n",
      " [ 3.]\n",
      " [ 2.]]\n",
      "[[ 1.99999574  1.00003267  1.00000321]\n",
      " [ 0.99999148 -1.99993466  2.00000641]]\n",
      "[[-0.99976328]\n",
      " [ 3.00001143]\n",
      " [ 2.00023945]]\n",
      "[[ 1.99999149  1.00006533  1.00000641]\n",
      " [ 0.99998297 -1.99986933  2.00001282]]\n",
      "[[-0.99952664]\n",
      " [ 3.00002286]\n",
      " [ 2.0004788 ]]\n",
      "[[ 1.99998723  1.00009799  1.00000961]\n",
      " [ 0.99997447 -1.99980402  2.00001923]]\n",
      "[[-0.99929009]\n",
      " [ 3.00003429]\n",
      " [ 2.00071807]]\n",
      "[[ 1.99998298  1.00013064  1.00001282]\n",
      " [ 0.99996597 -1.99973872  2.00002563]]\n",
      "[[-0.99905364]\n",
      " [ 3.00004571]\n",
      " [ 2.00095725]]\n",
      "[[ 1.99997874  1.00016328  1.00001602]\n",
      " [ 0.99995747 -1.99967344  2.00003203]]\n",
      "[[-0.99881727]\n",
      " [ 3.00005714]\n",
      " [ 2.00119634]]\n",
      "[[ 1.99997449  1.00019592  1.00001922]\n",
      " [ 0.99994898 -1.99960817  2.00003844]]\n",
      "[[-0.99858099]\n",
      " [ 3.00006856]\n",
      " [ 2.00143533]]\n",
      "[[ 1.99997025  1.00022854  1.00002242]\n",
      " [ 0.9999405  -1.99954291  2.00004484]]\n",
      "[[-0.99834479]\n",
      " [ 3.00007997]\n",
      " [ 2.00167424]]\n",
      "[[ 1.99996601  1.00026116  1.00002562]\n",
      " [ 0.99993202 -1.99947767  2.00005123]]\n",
      "[[-0.99810869]\n",
      " [ 3.00009139]\n",
      " [ 2.00191306]]\n",
      "[[ 1.99996177  1.00029378  1.00002882]\n",
      " [ 0.99992354 -1.99941245  2.00005763]]\n",
      "[[-0.99787267]\n",
      " [ 3.0001028 ]\n",
      " [ 2.00215179]]\n",
      "[[ 1.99995754  1.00032638  1.00003201]\n",
      " [ 0.99991507 -1.99934724  2.00006403]]\n",
      "[[-0.99763675]\n",
      " [ 3.00011421]\n",
      " [ 2.00239044]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x7fa097552f60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP(n_hidden=3, \n",
    "          epochs=10, \n",
    "          lr=0.001,\n",
    "          shuffle=False)\n",
    "\n",
    "X_train = np.array([[1, 2]])\n",
    "y_train = np.array([[0]])\n",
    "\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.99995754,  1.00032638,  1.00003201],\n",
       "       [ 0.99991507, -1.99934724,  2.00006403]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.weight_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99763675],\n",
       "       [ 3.00011421],\n",
       "       [ 2.00239044]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.weight_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
