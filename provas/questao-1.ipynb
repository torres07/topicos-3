{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, f_act='sigmoid', n_hidden=15, l2=0.0, epochs=20, lr=0.001, shuffle=True, minibatch_size=1, seed=None):\n",
    "        self.f_act = f_act\n",
    "        self.random = np.random.RandomState(seed)\n",
    "        self.n_hidden = n_hidden\n",
    "        # Lambda value for L2-regularization.\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatch_size = minibatch_size\n",
    "    \n",
    "    # activation_function (sigmoid)\n",
    "    def _sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(z, -250, 250)))\n",
    "        \n",
    "    # activation_function (tanh)\n",
    "    def _tanh(self, z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def _relu(self, z):\n",
    "        return np.maximum(z, 0)\n",
    "        \n",
    "    def _forward(self, X):\n",
    "        # net input of hidden layer\n",
    "        z_hidden = np.dot(X, self.weight_hidden)\n",
    "        \n",
    "        # activation of hidden layer\n",
    "        if self.f_act == 'relu':\n",
    "            activation_hidden = self._relu(z_hidden)\n",
    "        elif self.f_act == 'tanh':\n",
    "            activation_hidden = self._tanh(z_hidden)\n",
    "        else:\n",
    "            activation_hidden = self._sigmoid(z_hidden)\n",
    "        \n",
    "        # net input of out layer\n",
    "        z_out = np.dot(activation_hidden, self.weight_out)\n",
    "        \n",
    "        # activation of output layer\n",
    "        if self.f_act == 'relu':\n",
    "            activation_out = self._relu(z_out)\n",
    "        elif self.f_act == 'tanh':\n",
    "            activation_out = self._tanh(z_out)\n",
    "        else:\n",
    "            activation_out = self._sigmoid(z_out)\n",
    "        \n",
    "        return z_hidden, activation_hidden, z_out, activation_out\n",
    "        \n",
    "    def _compute_cost(self, y_enc, output):\n",
    "        # sse error with l2\n",
    "        l2_term = self.l2 * (np.sum(self.weight_hidden ** 2.0) + np.sum(self.weight_out ** 2.0))\n",
    "        cost = ((y_enc - output) ** 2).sum() + l2_term\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # only z_out matters\n",
    "        z_out = self._forward(X)[2]\n",
    "        y_pred = np.argmax(z_out, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    def _onehot(self, y, n_classes):\n",
    "        onehot = np.zeros((n_classes, y.shape[0]))\n",
    "        for idx, val in enumerate(y.astype(int)):\n",
    "            onehot[val, idx] = 1.\n",
    "        return onehot.T\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        n_output = np.unique(y_train).shape[0]\n",
    "        n_features = X_train.shape[1]\n",
    "        \n",
    "        self.weight_hidden = np.array([[2, 1 , 1], [1, -2, 2]])\n",
    "        self.weight_hidden = self.weight_hidden.astype(np.float64)\n",
    "        \n",
    "        self.weight_out = np.array([[-1], [3], [2]])\n",
    "        self.weight_out = self.weight_out.astype(np.float64)\n",
    "        \n",
    "        self.costs_ = []\n",
    "        \n",
    "        # new y encoded\n",
    "        y_train_enc = self._onehot(y_train, n_output)\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            \n",
    "            if self.shuffle:\n",
    "                self.random.shuffle(indices)\n",
    "            \n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "                \n",
    "                z_hidden, activation_hidden, z_out, activation_out = self._forward(X_train[batch_idx])\n",
    "\n",
    "                # backpropragation starts here\n",
    "                \n",
    "                sigma_out = activation_out - y_train_enc[batch_idx]\n",
    "                \n",
    "                # when using sigmoid as activation function\n",
    "                if self.f_act == 'relu':\n",
    "                    activation_derivate_hidden = activation_hidden\n",
    "                elif self.f_act == 'tanh':\n",
    "                    activation_derivate_hidden = (1.0 - (activation_hidden ** 2))\n",
    "                else:\n",
    "                    # sigmoid\n",
    "                    activation_derivate_hidden = activation_hidden * (1.0 - activation_hidden)\n",
    "                \n",
    "                sigma_hidden = np.dot(sigma_out, self.weight_out.T) * activation_derivate_hidden\n",
    "                grad_weight_hidden = np.dot(X_train[batch_idx].T, sigma_hidden)\n",
    "                grad_weight_out = np.dot(activation_hidden.T, sigma_out)\n",
    "                \n",
    "                # Regularization and weight updates\n",
    "                \n",
    "                # in hidden layer\n",
    "                delta_weight_hidden = (grad_weight_hidden + self.l2 * self.weight_hidden)\n",
    "                self.weight_hidden -= self.lr * delta_weight_hidden\n",
    "\n",
    "                # in output layer\n",
    "                delta_weight_out = (grad_weight_out + self.l2 * self.weight_out)\n",
    "                self.weight_out -= self.lr * delta_weight_out\n",
    "            \n",
    "            # for each epoch, after backpropagation ends evaluate training\n",
    "            z_hidden, activation_hidden, z_out, activation_out = self._forward(X_train)\n",
    "            \n",
    "            cost = self._compute_cost(y_enc=y_train_enc, output=activation_out)\n",
    "            self.costs_.append(cost)\n",
    "            \n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "epochs = [10, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1, 2]])\n",
    "y_train = np.array([[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quest√£o 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning-rate = 0.001 - epochs = 10 - weights:\n",
      "\n",
      "w_hidden:\n",
      "\n",
      "[[ 2.11791249  0.99900045  0.76604397]\n",
      " [ 1.23882363 -1.9980009   1.53208793]]\n",
      "\n",
      "w_out:\n",
      "\n",
      "[[-1.11318329]\n",
      " [ 2.99700135]\n",
      " [ 1.87836338]]\n",
      "\n",
      "learning-rate = 0.001 - epochs = 50 - weights:\n",
      "\n",
      "w_hidden:\n",
      "\n",
      "[[ 2.15290634  0.99501223  0.70661083]\n",
      " [ 1.32077599 -1.99002446  1.41322166]]\n",
      "\n",
      "w_out:\n",
      "\n",
      "[[-1.14717634]\n",
      " [ 2.98503669]\n",
      " [ 1.84060578]]\n",
      "\n",
      "learning-rate = 0.001 - epochs = 100 - weights:\n",
      "\n",
      "w_hidden:\n",
      "\n",
      "[[ 2.14192787  0.99004934  0.70337062]\n",
      " [ 1.31370773 -1.98009868  1.40674124]]\n",
      "\n",
      "w_out:\n",
      "\n",
      "[[-1.14124422]\n",
      " [ 2.97014802]\n",
      " [ 1.83158025]]\n",
      "\n",
      "learning-rate = 0.01 - epochs = 10 - weights:\n",
      "\n",
      "w_hidden:\n",
      "\n",
      "[[ 2.03831069  0.99004488  0.61498365]\n",
      " [ 1.10648674 -1.98008976  1.2299673 ]]\n",
      "\n",
      "w_out:\n",
      "\n",
      "[[-1.06747397]\n",
      " [ 2.97013464]\n",
      " [ 1.8004298 ]]\n",
      "\n",
      "learning-rate = 0.01 - epochs = 50 - weights:\n",
      "\n",
      "w_hidden:\n",
      "\n",
      "[[ 1.95482497  0.95120563  0.59519033]\n",
      " [ 1.05603306 -1.90241126  1.19038066]]\n",
      "\n",
      "w_out:\n",
      "\n",
      "[[-1.02222536]\n",
      " [ 2.85361688]\n",
      " [ 1.7322521 ]]\n",
      "\n",
      "learning-rate = 0.01 - epochs = 100 - weights:\n",
      "\n",
      "w_hidden:\n",
      "\n",
      "[[ 1.85496863  0.90479215  0.57175437]\n",
      " [ 0.99556081 -1.80958429  1.14350874]]\n",
      "\n",
      "w_out:\n",
      "\n",
      "[[-0.96785131]\n",
      " [ 2.71437644]\n",
      " [ 1.65104287]]\n",
      "\n",
      "learning-rate = 0.1 - epochs = 10 - weights:\n",
      "\n",
      "w_hidden:\n",
      "\n",
      "[[-0.21577462  0.90438208 -3.66320416]\n",
      " [-3.14469546 -1.80876415 -7.32640832]]\n",
      "\n",
      "w_out:\n",
      "\n",
      "[[-1.44326498]\n",
      " [ 2.71314623]\n",
      " [-0.47502897]]\n",
      "\n",
      "learning-rate = 0.1 - epochs = 50 - weights:\n",
      "\n",
      "w_hidden:\n",
      "\n",
      "[[-0.14434713  0.60500607 -2.45058013]\n",
      " [-2.10371245 -1.21001213 -4.90116026]]\n",
      "\n",
      "w_out:\n",
      "\n",
      "[[-0.96550351]\n",
      " [ 1.8150182 ]\n",
      " [-0.31778096]]\n",
      "\n",
      "learning-rate = 0.1 - epochs = 100 - weights:\n",
      "\n",
      "w_hidden:\n",
      "\n",
      "[[-0.08733089  0.36603234 -1.48261585]\n",
      " [-1.2727588  -0.73206468 -2.96523169]]\n",
      "\n",
      "w_out:\n",
      "\n",
      "[[-0.58413548]\n",
      " [ 1.09809702]\n",
      " [-0.19225941]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr_ in learning_rates:\n",
    "    for e_ in epochs:\n",
    "        mlp = MLP(n_hidden=3, epochs=e_, lr=lr_, shuffle=False, f_act='relu', l2=0.1)\n",
    "        mlp.fit(X_train, y_train)\n",
    "        \n",
    "        print('learning-rate = {} - epochs = {} - weights:\\n'.format(lr_, e_))\n",
    "        print('w_hidden:\\n')\n",
    "        print('{}\\n'.format(mlp.weight_hidden))\n",
    "        print('w_out:\\n')\n",
    "        print('{}\\n'.format(mlp.weight_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
